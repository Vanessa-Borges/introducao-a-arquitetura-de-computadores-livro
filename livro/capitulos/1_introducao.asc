== Introdução

.Objetivos do capítulo
____________________
Ao final deste capítulo você deverá ser capaz de:

* objetivo 1
* objetivo 2
* objetivo N
____________________

Neste lugar você deve apresentar o conteúdo em forma de diálogo.

NOTE: Para começar a escrever um novo capítulo, copie este arquivo e 
salve com outro nome (não utilize espaço no nome do arquivo). Em seguida,
atualize o arquivo *livro.asc* para incluir o novo arquivo criado. 
Consulte o manual.


=== O que é a Arquitetura de um Computador?

O termo arquitetura é principalmente utilizado na construção e decoração de edificações. Ele diz respeito à forma e a estrutura de uma construção. O termo refere-se à arte ou a técnica de projetar e edificar o ambiente habitado pelo ser humano. Na computação o termo foi adaptado para denominar a técnica (talvez até a arte também) de projetar e construir computadores. Nesse livro você não vai aprender a construir seu próprio computador. Para isso eu recomendo outros autores, John L. Hennessy, David A. Patterson e Andrew S. Tanenbaum. Esses autores produzem livros para engenheiros de computadores e acompanhá-los antes de se tornar um pode ser uma tarefa bastante árdua. Aqui você vai conhecer o computador por dentro e saber como ele funciona. Você não será capaz de construir um computador, mas saberá o suficiente para entender como os programas funcionam e até porque o computador para de funcionar as vezes, ou funciona lentamente, e que nessas situações, pressionar teclas do teclado rapidamente, ao mesmo tempo que move o mouse aleatoriamente, não faz o computador voltar a trabalhar novamente.

=== Por que estudar Arquitetura de Computadores?

É essencial que todos profissionais da Computação tenham pelo menos conhecimentos básicos de Arquitetura de Computadores. Saber como o computador funciona nos permitirá entender sua capacidade (e incapacidade) de resolver problemas, sobre como programá-los da melhor forma possível, como deixar o computador e os dados contidos neles mais seguros, como ganhar desempenho e o que faz ele ficar tão lento às vezes a ponto de querermos destrui-lo.
Então, estudar Arquitetura de Computadores é tão importante para um profissional de Computação, como estudar Anatomia é importante para um médico. Antes de iniciar qualquer estudo na Medicina, um médico precisa saber em detalhes o funcionamento do corpo humano. Quais são seus órgãos, como eles trabalham individualmente e como se relacionam para formar um sistema (digestivo, respiratório, motor etc.). Com a Arquitetura de Computadores é semelhante. Vamos aprender quais são os componentes de um computador, como eles funcionam e como eles trabalham em conjunto formando um sistema.Sem dúvidas o ser humano é a máquina mais perfeita já criada, mas vamos ver que o Computador é uma das máquinas mais incríveis que o homem já criou.


=== Um aluno de Licenciatura em Computação precisa estudar Arquitetura de Computadores?

Ao longo de minha experiência como professor de Arquitetura de Computadores para alunos de Licenciatura em Computação, eu sempre ouvi essa pergunta. “Por que precisamos estudar Arquitetura de Computadores?”. Espero que isso já esteja claro para você. Mas se ainda não estiver, aqui vai uma outra razão. Você será um licenciado e vai trabalhar no ensino e aprendizagem da Ciência da Computação. Como ensinar alguém sobre essa ciência se você não souber em detalhes como um computador funciona? Isso seria como um professor de Farmácia que não conhece bem a Química, ou um professor de Matemática que não conhece os números.


=== Sistemas Analógicos x Sistemas Digitais

Para sabermos a importância de um computador e sua forma de funcionamento, precisamos conhecer suas potencialidades e suas limitações. O computador é um dispositivo eletrônico digital. Isso significa que ele armazena, processa e gera dados na forma digital. Por outro lado, o computador não é capaz de processar dados analógicos. Eles antes precisam ser convertidos para digital para poderem ser utilizados por computadores. Mas o que venha a ser um dado analógico? Qualquer informação presente na natureza, como uma imagem, um som ou um cheiro, pode ser analisada em no mínimo duas componentes. Uma seria a sua intensidade e outra o tempo. A <<analog_digital, Figura>> a seguir apresenta essa representação, onde o sinal em forma de onda cinza seria a representação de um sinal analógico. 

[[analog_digital]]
.Sinal Analógico versus Sinal Digital
image::images/fig2.png[scaledwidth=“30%"]

Um som, por exemplo, é formado por vibrações no ar de diferentes intensidades (amplitudes) ao longo do tempo. Cada amplitude vai soar para nossos ouvidos como um tom diferente e alguns são até imperceptíveis aos nossos ouvidos. Por outro lado, como o computador é um dispositivo baseado em números, para que ele armazene um som em sua memória e possa fazer qualquer processamento sobre ele (gravar, transmitir, mixar), ele deve antes representá-lo na forma de números. Ai que está a dificuldade. As intensidades possíveis de um som são tantas que se aproximariam do infinito. Para tornar essa grandeza mais clara, imagine que pudéssemos emitir a intensidade do som emitido por um pássaro. Se em terminado momento dissermos que essa intensidade tem valor 5. Logo em seguida um outro som é emitido, medidos e constatamos que sua intensidade é 4. Até aí tudo bem! Mas o pássaro poderá em seguida emitir diversos sons que estariam entre 4 e 5, como 4,23, ou 4,88938, ou até uma dízima periódica, como 4,6666… Um ser humano, mesmo que não consiga medir a intensidade do canto do pássaro, consegue ouvi-lo, apreciá-lo e até repeti-lo com uma certa proximidade com alguns assobios. Mas o computador não trabalha assim! Antes de tudo, um computador teria que discretizar esses valores medidos, ou seja, passá-los do domínio dos números reais para o domínio dos inteiros. Assim, o que era 4 permanece 4, o que era 5, continua como 5, mas o que foi medido como 4,23 é convertido para 4, e o que era 4,88938 e 4,666 são convertidos para 5. Dessa forma, o computador passa a tratar com números reais e finitos. Um canto de um pássaro (ou até de uma orquestra sinfônica) pode ser armazenado e processador pelo computador. Na <<analog_digital, Figura>> apresentada, a onda quadrada representa um sinal digital.

Mas perceba que o som emitido pelo pássaro teve que ser modificado. Ele antes era complexo, rico e cheio de detalhes. Agora se tornou algo mais simples e reduzido. Houve uma perda de informação ao passarmos o dado do analógico para o digital. Processo semelhante ocorre quando outras informações da natureza são passadas para o computador, como uma imagem através de uma foto, ou uma cena através de um vídeo. Parte da informação deve ser ignorada para que possa ser armazenada em computadores. Você deve estar se perguntando então, quer dizer que imagens e sons analógicos possuem mais qualidade do que digitais? A resposta rigorosa para essa pergunta é, sim! Mas uma resposta mais consciente seria, as vezes! Isso porque a perda causada pela digitalização pode ser reduzida até níveis altíssimos que modo que nem o ouvido, nem a visão humana serão capazes de perceber.

Como exemplo de dados analógicos podemos citar tudo o que vem da natureza, som, imagem, tato, cheiro, enquanto que digitais são todos aqueles armazenados por dispositivos eletrônicos digitais, como computadores, celulares e TVs (exceto as antigas analógicas). Se uma foto digital, por exemplo, possui menos qualidade do que uma analógica, por que todos procuram apenas máquinas fotográficas digitais, transformando as analógicas quase em peças de museu? A resposta está na praticidade. Os computadores só entendem informações digitais. Uma máquina fotográfica, mesmo com qualidade inferior, vai nos permitir passar as fotos para o computador, compartilhar com os amigos, aplicar edições e melhorias, ampliar e copiar quantas vezes quisermos. Tarefas que antes eram impossíveis com máquinas analógicas. O mesmo pode ser refletido para músicas, documentos e livros. O mundo hoje é analógico!


=== O Transistor

O transistor é um componente eletrônico criado na década de 1950. Ele é o responsável pela revolução da eletrônica na década de 1960. Através dele foi possível desenvolver sistemas digitais extremamente pequenos. Todas funcionalidades de um computador são internamente executadas pela composição de milhões de transistores. Desde operações lógicas e aritméticas, até o armazenamento de dados em memórias (a exceção do disco rígido, CD, DVD e fitas magnéticas), tudo é feito pelos transistores.

Os primeiros eram fabricados na escala de micrômetros (latexmath:[10^-6] metros). Daí surgiram os termos microeletrônica e micro-tecnologia. Depois disso deu-se início a uma corrida tecnológica para se desenvolver transistores cada vez mais rápidos, menores e mais baratos. Essa revolução dura até hoje, mas foi mais forte nas décadas de 1980 e 1990. Foi emocionante acompanhar a disputa entre as empresas norte-americadas Intel e AMD para dominar o mercado de computadores pessoais. A cada 6 meses um novo processador era lançado por um delas, tomando da concorrente a posição de processador mais rápido do mercado. Poucos eram aqueles consumidores que conseguiam se manter a atualizados com tantos lançamentos.

O princípio básico é utilizar a eletrônica (corrente elétrica, resitência e tensão) para representar dados e depois poder executar operações com eles. A forma mais fácil de fazer isso foi primeiramente limitar os dados a apenas dois tipos. Zero e um. O sistema de numeração binário é muito mais fácil de representar com dispositivos eletrônicos do que o decimal, por exemplo. O transistor possui dois estados. Ou ele está carregado, ou está descarregado, assim como uma pilha. Isso facilmente pode ser mapeado para o bit 1 (carregado) e o bit (0). O revolucionário, diferente de uma pilha, foi possibilitar que esse estado pudesse ser mudado eletronicamente a qualquer momento e de forma muito rápida.Assim, com 8 transistores em paralelo, eu posso representar, por exemplo um número de 8 bits. Posso mudar seus valores mudando suas cargas, e posso ler seus valores chegando se cada um possui, ou não carga. Esse é o princípio básico de construção de uma memória.

De forma semelhante, é possível integrar transistores para que os mesmos executem operações lógicas e aritméticas. As portas lógicas estudadas por você em Introdução à Computação são todas fabricadas utilizando transistores.Quanto menores são os transistores, mais dados podem ser armazenados por área. Ao mesmo tempo, transistores menores guardam menos carga. Isso torna mais rápido o processo de carregamento e descarregamento, que, por consequência, torna o processamento e armazenamento de dados muito mais rápidos também.
Com a evolução da nanoeletrônica, os transistores são tão pequenos que possibilitou a construção de memórias de 1GB (um giga byte) do tamanho da unha da mão de um adulto. Para ser ter uma ideia, 1 Giga é a abreviação de latexmath:[10^9], ou seja, um bilhão. Um byte são 8 bits. Então, uma memória de 1GB possui, pelo menos, 8 bilhões de transistores. Os processadores também se tornaram bastante velozes com a miniaturização dos transistores. Os processadores atuais trabalham na frequência de GHz (Giga Hertz), ou seja, na casa de bilhões de ciclos por segundo (diferente de operações por segundo). Isso é muito rápido!

[[mosfet]]
.Estrutura de um transistor tipo MOSFET
image::images/fig1.png[scaledwidth="60%"]

Na <<mosfet, Figura>> anterior é apresentada a estrutura de um transistor MOSFET. Esse transistor é o mais utilizado para se construir sistemas eletrônicos digitais, como os computadores. O nome vem da abreviação de ‘Metal-Oxide Semiconductor Field-Effect Transistor’. Vamos ver o que significa cada palavra dessas, e isso nos ajudará a conhecer um pouco mais o MOSFET e sua relevância. O termo MOS (‘Metal-Oxide Semiconductor’) vem dos materiais utilizados para compor um MOSFET, que são principalmente, óxido metálico e semicondutor.

Semicondutores são materiais que possuem propriedades que nem os permitem classificar como condutor, nem como isolante. Em algumas condições ele age como um isolante, e em outras, como um condutor. O semicondutor mais utilizado em transistores é o silício (símbolo Si na Tabela Periódica). Em condições ambientes, o silício age como um isolante, mas se misturado a outros materiais, ele pode se tornar um condutor até a intensidade desejada. 
[NOTE]
O Silício se tornou tão importante que modificou toda uma região da Califórnia nos Estados Unidos na década de 1950, tornando-a uma das mais promissoras do mundo até hoje. Essa região abrigou e abriga as mais importantes empresas do ramo de projeto de computadores, como Intel, AMD, Dell, IBM e Apple, e depois de softwares que iriam executar nesses computadores, como Microsoft, Oracle e Google. Essa região é chamada de Vale do Silício.

No transistor da <<mosfet, Figura>> a cor verde representa um cristal de silício que foi dopado com cargas negativas. Já a cor vermelha, representa a parte que foi dopada com cargas positivas. Na situação normal uma corrente elétrica aplicada no Dreno (Drain) consegue produz percorrer o estreito canal negativo e seguir até a Fonte (Source). Nessa condição dizemos que o transistor está ativo. Porém, for aplicada uma tensão negativa na Porta (Gate), as cargas positivas da região P (em vermelho) serão atraídas para mais próximo da Porta, e isso irá fechar o canal por onde passava a corrente elétrica. Nesse caso, dizemos que o transistor está inativo.

Por que isso tudo nos interessa? Quando o transistor está ativo, ele pode ser visto com o valor 1, e quando inativo, ele pode ser visto com o valor 0. Assim, temos a menor memória possível de ser construída. Quando quisermos que ela guarde o valor 1, basta desligar a tensão da Porta e aplicar uma corrente no Dreno. Já quando quisermos que ele armazene o valor 0, precisamos aplicar uma corrente na Porta e fechar o canal. Então, a memória de 8 bilhões de bits, pode ser elaborada com 8 bilhões de transistores como esses.

Agora conhecemos o primeiro aspecto que faz dos transistores essenciais para o entendimento do computador. Eles são usados para a construção de memórias. Memórias feitas a base de transistores são chamadas também de Memórias de Estado Sólido. Mas há outras, não tão eficientes e miniaturizadas, como memórias ópticas e magnéticas. O importante percebermos é que quanto menores pudermos construir esses transistores, melhor. O processo de abertura e fechamento do canal não é instantâneo. Ele leva um curtíssimo tempo, mas quando somados os tempos de todos os bilhões de transistores, ele passa a se tornar relevante. Quanto menor ele for, mais estreito é o canal e, portanto, mais rápido ele liga e desliga, da mesma forma, menor será a distância entre o Dreno e a Fonte, levando também menos tempo para os elétrons deixarem o Dreno em direção à fonte. Isso tudo fará a memória mais rápida. Transistores pequenos também possibilitam que mais dados sejam armazenados por área. É por isso que hoje enormes capacidades de armazenamento são disponíveis em dispositivos tão reduzidos, como são os exemplos de pen-drives e cartões de memória.

Os transistores também são usados para executar operações lógicas e aritméticas. A carga retirada de um transistor pode servir para alimentar um outro e que, se combinados de forma correta, podem executar as operações lógicas básicas, E, OU, NÃO e as aritméticas, adição, subtração, divisão e multiplicação. Com isso, os transistores não apenas podem ser utilizados para armazenar dados, mas como executar operações lógicas e aritméticas sobre esses dados. Isso é fantástico e vem revolucionado todo o mundo. Não só na Ciência da Computação, mas como também em todas áreas do conhecimento. O que seria da humanidade hoje sem o computador? Sem o telefone celular? Sem os satélites?

=== A Lei de Moore

Durante os anos de 1950 e 1965, a industrias do Vale do Silício disputavam pelo domínio do recém-surgido mercado da computação e eletrônica. Naquela época ainda não havia surgido o termo TIC (Técnologia da Informação e Comunicação), mas ele seria mais apropriado para definir o nicho de clientes e serviços que eles disputavam. Eles dominavam a produção de circuitos eletrônicos digitais, dominados pela Intel e AMD, a produção de computadores e equipamentos de comunicação, como a Dell, Apple, IBM, HP e CISCO, além da indústria e software e serviços, como a Apple, Microsoft e, mais tarde, a Google. A disputa era grande e nem sempre leal.

[NOTE]
Assista ao filme “Piratas do Vale do Silício” (1999) e tenha uma ideia de como essa guerra estava longe de ser limpa. 

Entretanto, não se sabia naquela época onde essa disputa ia parar, nem quem seriam os vencedores, nem mesmo, se haveria sequer vencedores. Até um dos sócios e presidente da Intel, Gordon Moore,  lançou um trabalho minucioso onde ele destacava a experiência que ele adquiriu ao longe de alguns anos trabalhando na indústria de fabricação de processadores e circuitos para computadores. Ele percebeu que, sempre a indústria avançava em sua tecnologia e conseguia reduzir o tamanho de cada transistor de um circuito integrado, os computadores tornavam-se também muito mais velozes do que antes. Porém, essa redução no tamanho dos transistores requer uma total atualização nos equipamentos da indústria, tornando os equipamentos anteriores obsoletos. Assim, só seria viável a evolução para transistores menores se o lucro da empresa fosse o suficiente para pagar todas essas despesas. Por outro lado, ele também percebeu que os computadores e equipamentos mais obsoletos ainda possuíam mercado aberto em países menos desenvolvidos economicamente. Ele concluiu então que a indústria seria sim capaz de continuar evoluindo na redução do tamanho dos transistores porque os novos computadores, sendo tornando mais velozes, seriam tão mais eficientes e atrativos, que todos os clientes, principalmente as empresas, fariam de tudo para trocar seus computadores antigos por novos, afim de se tornarem cada vez mais produtivos.

Além dessa análise de mercado, ele analisou como o processo industrial era concebido e como os novos computadores se beneficiariam da redução do tamanho dos transistores. Ao final, ele afirmou que “A cada ano a quantidade de transistores por chip irá dobrar de tamanho, sem alteração em seu preço”. Essa frase pode ser interpretada também pelas consequências da quantidade de transistores por chip. Ou seja, a cada ano, com o dobro dos transistores, os chips se tornarão duas vezes mais rápidos. Um exemplo mais comum de chip são os processadores dos computadores. Então, por consequência, os computadores irão dobrar sua velocidade de processamento a cada ano, e ainda vão permanecer com o mesmo preço. 

Naquela época essa era uma afirmação muito forte e ambiciosa. Muitos receberam esse estudo com cautela. Mas não demorou muito para todos perceberem que as previsões de Moore estavam se realizando. Foi tanto, e o trabalho dele foi depois chamado de “Lei de Moore” e ela ainda é válida até os dias de hoje. Na <<moore, Figura>> a seguir é possível perceber como a quantidade de transistores por processadores cresceu dos anos 1970 até por volta de 2003 (linha contínua). É possível ver que ela não se afastou muito das previsões de Moore (linha tracejada).

[[moore]]
.Evolução do número de transistores nos processadores em comparação com a Lei de Moore
image::images/fig3.png[scaledwidth="60%"]

A Lei de Moore se tornou tão importante que ela não é usada apenas como uma meta a ser buscada e batida a cada ano, mas também como um meio para se verificar se a indústria está evoluindo na velocidade esperada. Apesar de Moore está muito correto em suas previsões, todos sabem, inclusive ele próprio, que esse crescimento não vai durar para sempre. Os transistores hoje estão na escala de 25 nanometros. Essa é a mesma escala de alguns vírus e bactérias. Reduzir mais do que isso está se tornando cada vez mais difícil. Pesquisadores e cientistas buscam outras formas de fazer com que os computadores continuem evoluindo em sua velocidade e reduzindo seu tamanho. Alguns pensam na substituição de transistores de Silício por outros materiais, como Grafeno. Outros até são mais radicais e defendem que a forma de computação deve mudar, talvez através de Computadores Quânticos ou de Bio-Computadores.

É muito importante para toda a humanidade que os computadores continuem evoluindo. A redução do tamanho dos computadores, aliada ao aumento de desempenho e sem o crescimento dos preços, permitiu que todas as ciência evoluíssem ao mesmo tempo, com a mesma velocidade. A metereologia, a medicina, as engenharias e até as Ciências Humanas avançaram sempre em conjunto com o avanço da computação. Para se ter um exemplo, foi a evolução dos transistores que permitiu que computadores se comunicassem numa velocidade tão grande que permitiu a formação da rede mundial de computadores, a Internet. Qualquer pessoa hoje consegue em poucos milissegundos fazer uma pesquisa por informações que estão do outro lado do planeta. Algo que antes só era possível viajando até bibliotecas distantes e cheirando bastante mofo e poeira. Hoje, ter em casa bilhões de bytes (Giga bytes) armazenados num minúsculo cartão de memória, é algo corriqueiro. A informação está hoje disponível numa escala tão grande e numa velocidade tão intensa que parece que mais nada é impossível para a humanidade. Após a Revolução Industrial do século XVIII que substitui os trabalhadores braçais por máquinas, o século XX, puxado pela evolução dos transistores, passou pelo o que muitos consideram a Revolução da Informação e o século XXI, já é considerado a “Era do Conhecimento”.




=== A evolução dos computadores

Texto aqui!!!!

=== A Arquitetura de Von Neumann

Texto aqui!!!!

=== Recapitulando

Revisão do que foi aprendido.

Reserve o último parágrafo para realizar uma *ponte* para o próximo capítulo.

=== Atividades

* Texto da atividade.
* Texto da atividade.
* Texto da atividade.

CAUTION: *Sempre termine os arquivos com uma linha em branco*, caso contrário você poderá encontrar erros inesperados.

// Sempre terminar o arquivo com uma nova linha.
