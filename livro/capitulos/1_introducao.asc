== Introdução

.Objetivos do capítulo
____________________
Ao final deste capítulo você deverá ser capaz de:

* Definir o que é a Arquitetura de Computadores e sua relevância
* Apresentar a Arquitetura Geral de um computador e suas principais 
operações
* Diferencias sistemas digitais de analógicos
* Apresentar o funcionamento de um transistor e sua relevância para 
a industria de dispositivos digitais
* Destacar a Lei de Moore e seu impacto para a evolução da 
industria de dispositivos eletrônicos digitais
* Identificar os principais fatos da evolução dos computadores

____________________

Você sabe o que é Arquitetura de computadores? Você já se 
perguntou porque precisa estudar Arquitetura de Computadores? Esse 
capítulo nós vamos aprender que essa é uma das principais 
disciplinas da Ciência da Computação. Não só isso, foi a 
Arquitetura de Computadores que permitiu que a humanidade avançasse 
em todos os aspectos da ciência, saúde e tecnologia. Ao final desse 
capítulo, espero que você concorde comigo.


=== O que é a Arquitetura de um Computador?

O termo arquitetura é principalmente utilizado na construção e 
decoração de edificações. Ele diz respeito à forma e a estrutura 
de uma construção. O termo refere-se à arte ou a técnica de 
projetar e edificar o ambiente habitado pelo ser humano. Na 
computação o termo foi adaptado para denominar a técnica (talvez 
até a arte também) de projetar e construir computadores. Nesse 
livro você não vai aprender a construir seu próprio computador. 
Para isso eu recomendo outros autores, John L. Hennessy, David A. 
Patterson e Andrew S. Tanenbaum. Esses autores produzem livros para 
engenheiros de computadores e acompanhá-los antes de se tornar um 
pode ser uma tarefa bastante árdua. Aqui você vai conhecer o 
computador por dentro e saber como ele funciona. Você não será 
capaz de construir um computador, mas saberá o suficiente para 
entender como os programas funcionam e até porque o computador para 
de funcionar as vezes, ou funciona lentamente, e que nessas 
situações, pressionar teclas do teclado rapidamente, ao mesmo tempo 
que move o mouse aleatoriamente, não faz o computador voltar a 
trabalhar novamente.

=== Por que estudar Arquitetura de Computadores?

É essencial que todos profissionais da Computação tenham pelo 
menos conhecimentos básicos de Arquitetura de Computadores. Saber 
como o computador funciona nos permitirá entender sua capacidade (e 
incapacidade) de resolver problemas, sobre como programá-los da 
melhor forma possível, como deixar o computador e os dados contidos 
neles mais seguros, como ganhar desempenho e o que faz ele ficar tão 
lento às vezes a ponto de querermos destrui-lo.
Então, estudar Arquitetura de Computadores é tão importante para 
um profissional de Computação, como estudar Anatomia é importante 
para um médico. Antes de iniciar qualquer estudo na Medicina, um 
médico precisa saber em detalhes o funcionamento do corpo humano. 
Quais são seus órgãos, como eles trabalham individualmente e como 
se relacionam para formar um sistema (digestivo, respiratório, motor 
etc.). Com a Arquitetura de Computadores é semelhante. Vamos 
aprender quais são os componentes de um computador, como eles 
funcionam e como eles trabalham em conjunto formando um sistema.
Sem dúvidas o ser humano é a máquina mais perfeita já criada, mas 
vamos ver que o Computador é uma das máquinas mais incríveis que o 
homem já criou.


=== Um aluno de Licenciatura em Computação precisa estudar 
Arquitetura de Computadores?

Ao longo de minha experiência como professor de Arquitetura de 
Computadores para alunos de Licenciatura em Computação, eu sempre 
ouvi essa pergunta. ``Por que precisamos estudar Arquitetura de 
Computadores?”. Espero que isso já esteja claro para você. Mas se 
ainda não estiver, aqui vai uma outra razão. Você será um 
licenciado e vai trabalhar no ensino e aprendizagem da Ciência da 
Computação. Como ensinar alguém sobre essa ciência se você não 
souber em detalhes como um computador funciona? Isso seria como um 
professor de Farmácia que não conhece bem a Química, ou um 
professor de Matemática que não conhece os números.


=== Arquitetura geral

Hoje em dia há muitos tipos de computadores e diversas arquiteturas. 
Elas são frutos de muitos estudos, pesquisas e avanços 
tecnológicos. Mas todos computadores compartilham uma arquitetura 
comum. Essa arquitetura é o que separa um computador de uma 
calculadora de bolso, de um aparelho de televisão ou um relógio de 
pulso. Essa arquitetura é apresentada na <<fig_arquitetura>>.

[[fig_arquitetura]]
.Arquitetura básica de um computador
image::images/fig_arquitetura.jpg[scaledwidth=“30%"] 

Todo computador possui uma Unidade Central de Processamento, ou, do 
inglês, *Central Processing Unit* (CPU) e uma Memória Principal. 
Todos os dados a serem processados pela CPU, para operações 
lógicas e aritméticas, precisam estar na memória. Da memória os 
dados são transferidos para a CPU através de fios paralelos de 
comunicação, chamados de Barramento de Dados. Entretando, a CPU 
não toma decisões por si própria. Ela não sabe que dados deve 
trazer da memória, muito menos que operação executar com eles. 
Para isso, ela precisa que instruções, também armazenadas na 
memória, sejam trazidas para a CPU através do Barramento de 
Endereço. Cada instrução informa para a CPU que operação ela 
deve executar, com quais dados e o que ela deve fazer com o resultado 
da operação. 

Para poder se localizar, a memória é organizada em endereços. 
Todos os dados e as instruções são localizadas através desses 
endereços. Cada instrução indica para a CPU que dados devem ser 
transferidos e processados através dos endereços desses dados. Esse 
endereço é transferido para a memória pela CPU através do 
Barramento de Endereço. A memória localiza o tal dado e o transfere 
para a CPU via Barramento de Dados. As instruções são 
desenvolvidas pelo programador, através de linguagens de 
programação. As ferramentas de compilação transformam os 
programas escritos em linguagens de alto nível, como C, Java e 
Phython, em instruções de máquina, que são finalmente copiadas 
para a memória no momento em que precisam ser executadas. Cada 
instrução é armazenada em um endereço diferente da memória. Na 
execução normal, a CPU passa para a memória, via Barramento de 
Endereço, o endereço da primeira instrução do programa, a 
memória transfere a instrução pelo Barramento de Instrução, a 
CPU a executa e, em seguida, solicita a instrução do endereço 
seguinte. Assim, os programas são executados sempre de forma 
sequencial, a não ser que uma instrução especial solicite que ela 
salte para uma instrução que não seja a consecutiva. Isso é o 
caso quando há instruções condicionais (como o ``se” ou 
``if”), instruções de repetição (como ``while” e o ``for”), 
ou chamadas a sub-programas, ou mesmo, por ordem do Sistema 
Operacional, para que o programa pare de executar para que um outro 
tome seu lugar.

As memórias são, quase sempre, muito mais lentas do que as CPUs. 
Isso exigiu, ao longo dos anos, que as CPUs possuíssem também uma 
porção interna de memória muito rápida, chamada Memória Cache. A 
tecnologia que permite essas memórias serem mais rápidas, tornam-as 
também muito caras. Por isso que sua capacidade geralmente é muito 
limitada. Para acelerar ainda mais, elas são instaladas dentro das 
CPUs. Todos os dados e instruções transferidos da Memória 
Principal para a CPU são salvos também na Cache. Como a Cache não 
é capaz de guardar todos os dados da Memória Principal, apenas os 
dados mais recentes transferidos para a CPU permanecem na Cache. 
Técnicas muito avançadas são aplicadas para que se consiga, no 
máximo possível, manter os dados mais importantes daquele instante 
na Memória Cache.

A CPU também é responsável por enviar sinais de controle aos 
outros dispositivos do computador, como periféricos, dispositivos de 
entrada e saída, e memórias externas. Esse sinais são enviados 
quando uma instrução dá ordem para tal. Por exemplo, quando uma 
instrução pede que uma mensagem sem impressa na tela, a CPU, ao 
receber e executar essa instrução, envia para o controle do monitor 
que imprima na tela a mensagem contida o endereço que também foi 
passada pela instrução.

É esse comportamento que diferencia um computador de outros 
dispositivos eletrônicos mais simples. A essência da CPU não é 
muito diferente de uma calculadora de bolso. Ela executa operações 
lógicas e aritméticas. Entretanto, no projeto do computador, o 
papel do homem foi substituído pela programação. Todas 
instruções das tarefas que a CPU precisa executar são armazenadas 
na memória e, a partir de então, a CPU pode trabalhar sem qualquer 
interferência externa. Com a programação, a CPU pode também 
executar tarefas diversas, desde simulações, jogos, tocar músicas 
e vídeos etc. Simplificando, o computador é uma máquina 
programável e de propósito geral. 

==== Operações básicas

Todos computadores executam três operações básicas:

* Movimentação de dados
* Processamentos de dados
* Armazenamento de dados

A movimentação de dados é a transferência de um dado de um ponto 
para outro do computador. Pode ser de um endereço de memória para 
outro, de um dispositivo de entrada para a memória, ou da memória 
para um dispositivo de saída. O processamento de dados ocorre quando 
a CPU recebe um determinado dado e executa uma operação que o 
modifica de alguma forma. Já as operações de armazenamento ocorre 
quando a CPU precisa registrar um dado em algum local específico, 
como salvar um dado no disco rígido, ou num pendrive, ou mesmo na 
Memória Principal.

=== Sistemas Analógicos x Sistemas Digitais

Para sabermos a importância de um computador e sua forma de 
funcionamento, precisamos conhecer suas potencialidades e suas 
limitações. O computador é um dispositivo eletrônico digital. 
Isso significa que ele armazena, processa e gera dados na forma 
digital. Por outro lado, o computador não é capaz de processar 
dados analógicos. Eles antes precisam ser convertidos para digital 
para poderem ser utilizados por computadores. Mas o que venha a ser 
um dado analógico? Qualquer informação presente na natureza, como 
uma imagem, um som ou um cheiro, pode ser analisada em no mínimo 
duas componentes. Uma seria a sua intensidade e outra o tempo. A 
<<fig_analog_digital>> a seguir apresenta essa representação, onde 
o sinal em forma de onda cinza seria a representação de um sinal 
analógico. 

[[fig_analog_digital]]
.Sinal Analógico versus Sinal Digital
image::images/fig2.png[scaledwidth=“30%"]

Um som, por exemplo, é formado por vibrações no ar de diferentes 
intensidades (amplitudes) ao longo do tempo. Cada amplitude vai soar 
para nossos ouvidos como um tom diferente e alguns são até 
imperceptíveis aos nossos ouvidos. Por outro lado, como o computador 
é um dispositivo baseado em números, para que ele armazene um som 
em sua memória e possa fazer qualquer processamento sobre ele 
(gravar, transmitir, mixar), ele deve antes representá-lo na forma 
de números. Ai que está a dificuldade. As intensidades possíveis 
de um som são tantas que se aproximariam do infinito. Para tornar 
essa grandeza mais clara, imagine que pudéssemos emitir a 
intensidade do som emitido por um pássaro. Se em terminado momento 
dissermos que essa intensidade tem valor 5. Logo em seguida um outro 
som é emitido, medidos e constatamos que sua intensidade é 4. Até 
aí tudo bem! Mas o pássaro poderá em seguida emitir diversos sons 
que estariam entre 4 e 5, como 4,23, ou 4,88938, ou até uma dízima 
periódica, como 4,6666… Um ser humano, mesmo que não consiga 
medir a intensidade do canto do pássaro, consegue ouvi-lo, 
apreciá-lo e até repeti-lo com uma certa proximidade com alguns 
assobios. Mas o computador não trabalha assim! Antes de tudo, um 
computador teria que discretizar esses valores medidos, ou seja, 
passá-los do domínio dos números reais para o domínio dos 
inteiros. Assim, o que era 4 permanece 4, o que era 5, continua como 
5, mas o que foi medido como 4,23 é convertido para 4, e o que era 
4,88938 e 4,666 são convertidos para 5. Dessa forma, o computador 
passa a tratar com números reais e finitos. Um canto de um pássaro 
(ou até de uma orquestra sinfônica) pode ser armazenado e 
processador pelo computador. Na <<fig_analog_digital>> apresentada, a 
onda quadrada representa um sinal digital.

Mas perceba que o som emitido pelo pássaro teve que ser modificado. 
Ele antes era complexo, rico e cheio de detalhes. Agora se tornou 
algo mais simples e reduzido. Houve uma perda de informação ao 
passarmos o dado do analógico para o digital. Processo semelhante 
ocorre quando outras informações da natureza são passadas para o 
computador, como uma imagem através de uma foto, ou uma cena 
através de um vídeo. Parte da informação deve ser ignorada para 
que possa ser armazenada em computadores. Você deve estar se 
perguntando então, quer dizer que imagens e sons analógicos possuem 
mais qualidade do que digitais? A resposta rigorosa para essa 
pergunta é, sim! Mas uma resposta mais consciente seria, as vezes! 
Isso porque a perda causada pela digitalização pode ser reduzida 
até níveis altíssimos que modo que nem o ouvido, nem a visão 
humana serão capazes de perceber.

Como exemplo de dados analógicos podemos citar tudo o que vem da 
natureza, som, imagem, tato, cheiro, enquanto que digitais são todos 
aqueles armazenados por dispositivos eletrônicos digitais, como 
computadores, celulares e TVs (exceto as antigas analógicas). Se uma 
foto digital, por exemplo, possui menos qualidade do que uma 
analógica, por que todos procuram apenas máquinas fotográficas 
digitais, transformando as analógicas quase em peças de museu? A 
resposta está na praticidade. Os computadores só entendem 
informações digitais. Uma máquina fotográfica, mesmo com 
qualidade inferior, vai nos permitir passar as fotos para o 
computador, compartilhar com os amigos, aplicar edições e 
melhorias, ampliar e copiar quantas vezes quisermos. Tarefas que 
antes eram impossíveis com máquinas analógicas. O mesmo pode ser 
refletido para músicas, documentos e livros. O mundo hoje é 
digital, e não há como fugirmos disso!


=== O Transistor

O transistor é um componente eletrônico criado na década de 1950. 
Ele é o responsável pela revolução da eletrônica na década de 
1960. Através dele foi possível desenvolver sistemas digitais 
extremamente pequenos. Todas funcionalidades de um computador são 
internamente executadas pela composição de milhões de 
transistores. Desde operações lógicas e aritméticas, até o 
armazenamento de dados em memórias (a exceção do disco rígido, 
CD, DVD e fitas magnéticas), tudo é feito pelos transistores.

Os primeiros eram fabricados na escala de micrômetros 10^-6^ 
metros). Daí surgiram os termos microeletrônica e micro-tecnologia. 
Depois disso deu-se início a uma corrida tecnológica para se 
desenvolver transistores cada vez mais rápidos, menores e mais 
baratos. Essa revolução dura até hoje, mas foi mais forte nas 
décadas de 1980 e 1990. Foi emocionante acompanhar a disputa entre 
as empresas norte-americadas Intel e AMD para dominar o mercado de 
computadores pessoais. A cada 6 meses um novo processador era 
lançado por um delas, tomando da concorrente a posição de 
processador mais rápido do mercado. Poucos eram aqueles consumidores 
que conseguiam se manter a atualizados com tantos lançamentos.

O princípio básico é utilizar a eletrônica (corrente elétrica, 
resistência e tensão) para representar dados e depois poder 
executar operações com eles. A forma mais fácil de fazer isso foi 
primeiramente limitar os dados a apenas dois tipos. Zero e um. O 
sistema de numeração binário é muito mais fácil de representar 
com dispositivos eletrônicos do que o decimal, por exemplo. O 
transistor possui dois estados. Ou ele está carregado, ou está 
descarregado, assim como uma pilha. Isso facilmente pode ser mapeado 
para o bit 1 (carregado) e o bit (0). O revolucionário, diferente de 
uma pilha, foi possibilitar que esse estado pudesse ser mudado 
eletronicamente a qualquer momento e de forma muito rápida.

Assim, com 8 transistores em paralelo, eu posso representar, por 
exemplo um número de 8 bits. Posso mudar seus valores mudando suas 
cargas, e posso ler seus valores chegando se cada um possui, ou não 
carga. Esse é o princípio básico de construção de uma memória.

De forma semelhante, é possível integrar transistores para que os 
mesmos executem operações lógicas e aritméticas. As portas 
lógicas estudadas por você em Introdução à Computação são 
todas fabricadas utilizando transistores.

Quanto menores são os transistores, mais dados podem ser armazenados 
por área. Ao mesmo tempo, transistores menores guardam menos carga. 
Isso torna mais rápido o processo de carregamento e descarregamento, 
que, por consequência, torna o processamento e armazenamento de 
dados muito mais rápidos também.

Com a evolução da nanoeletrônica, os transistores são tão 
pequenos que possibilitou a construção de memórias de 1GB (um giga 
byte) do tamanho da unha da mão de um adulto. Para ser ter uma 
ideia, 1 Giga é a abreviação de 10^9^, ou seja, um bilhão. Um 
byte são 8 bits. Então, uma memória de 1GB possui, pelo menos, 8 
bilhões de transistores. Os processadores também se tornaram 
bastante velozes com a miniaturização dos transistores. Os 
processadores atuais trabalham na frequência de GHz (Giga Hertz), ou 
seja, na casa de bilhões de ciclos por segundo (diferente de 
operações por segundo). Isso é muito rápido!

[[fig_mosfet]]
.Estrutura de um transistor tipo MOSFET
image::images/fig1.png[scaledwidth="60%"]

Na <<fig_mosfet>> anterior é apresentada a estrutura de um 
transistor MOSFET. Esse transistor é o mais utilizado para se 
construir sistemas eletrônicos digitais, como os computadores. O 
nome vem da abreviação de ``Metal-Oxide Semiconductor Field-Effect 
Transistor”. Vamos ver o que significa cada palavra dessas, e isso 
nos ajudará a conhecer um pouco mais o MOSFET e sua relevância. O 
termo MOS (``Metal-Oxide Semiconductor”) vem dos materiais 
utilizados para compor um MOSFET, que são principalmente, óxido 
metálico e semicondutor.

Semicondutores são materiais que possuem propriedades que nem os 
permitem classificar como condutor, nem como isolante. Em algumas 
condições ele age como um isolante, e em outras, como um condutor. 
O semicondutor mais utilizado em transistores é o silício (símbolo 
Si na Tabela Periódica). Em condições ambientes, o silício age 
como um isolante, mas se misturado a outros materiais, ele pode se 
tornar um condutor até a intensidade desejada. 

[NOTE]
O Silício se tornou tão importante que modificou toda uma região 
da Califórnia nos Estados Unidos na década de 1950, tornando-a uma 
das mais promissoras do mundo até hoje. Essa região abrigou e 
abriga as mais importantes empresas do ramo de projeto de 
computadores, como Intel, AMD, Dell, IBM e Apple, e depois de 
softwares que iriam executar nesses computadores, como Microsoft, 
Oracle e Google. Essa região é chamada de Vale do Silício.

No transistor da <<fig_mosfet>> a cor verde representa um cristal de 
silício que foi dopado com cargas negativas. Já a cor vermelha, 
representa a parte que foi dopada com cargas positivas. Na situação 
normal uma corrente elétrica aplicada no Dreno (Drain) consegue 
produz percorrer o estreito canal negativo e seguir até a Fonte 
(Source). Nessa condição dizemos que o transistor está ativo. 
Porém, for aplicada uma tensão negativa na Porta (Gate), as cargas 
positivas da região P (em vermelho) serão atraídas para mais 
próximo da Porta, e isso irá fechar o canal por onde passava a 
corrente elétrica. Nesse caso, dizemos que o transistor está 
inativo.

Por que isso tudo nos interessa? Quando o transistor está ativo, ele 
pode ser visto com o valor 1, e quando inativo, ele pode ser visto 
com o valor 0. Assim, temos a menor memória possível de ser 
construída. Quando quisermos que ela guarde o valor 1, basta 
desligar a tensão da Porta e aplicar uma corrente no Dreno. Já 
quando quisermos que ele armazene o valor 0, precisamos aplicar uma 
corrente na Porta e fechar o canal. Então, a memória de 8 bilhões 
de bits, pode ser elaborada com 8 bilhões de transistores como esses.

Agora conhecemos o primeiro aspecto que faz dos transistores 
essenciais para o entendimento do computador. Eles são usados para a 
construção de memórias. Memórias feitas a base de transistores 
são chamadas também de Memórias de Estado Sólido. Mas há outras, 
não tão eficientes e miniaturizadas, como memórias ópticas e 
magnéticas. O importante percebermos é que quanto menores pudermos 
construir esses transistores, melhor. O processo de abertura e 
fechamento do canal não é instantâneo. Ele leva um curtíssimo 
tempo, mas quando somados os tempos de todos os bilhões de 
transistores, ele passa a se tornar relevante. Quanto menor ele for, 
mais estreito é o canal e, portanto, mais rápido ele liga e 
desliga, da mesma forma, menor será a distância entre o Dreno e a 
Fonte, levando também menos tempo para os elétrons deixarem o Dreno 
em direção à fonte. Isso tudo fará a memória mais rápida. 
Transistores pequenos também possibilitam que mais dados sejam 
armazenados por área. É por isso que hoje enormes capacidades de 
armazenamento são disponíveis em dispositivos tão reduzidos, como 
são os exemplos de pen-drives e cartões de memória.

Os transistores também são usados para executar operações 
lógicas e aritméticas. A carga retirada de um transistor pode 
servir para alimentar um outro e que, se combinados de forma correta, 
podem executar as operações lógicas básicas, E, OU, NÃO e as 
aritméticas, adição, subtração, divisão e multiplicação. Com 
isso, os transistores não apenas podem ser utilizados para armazenar 
dados, mas como executar operações lógicas e aritméticas sobre 
esses dados. Isso é fantástico e vem revolucionado todo o mundo. 
Não só na Ciência da Computação, mas como também em todas 
áreas do conhecimento. O que seria da humanidade hoje sem o 
computador? Sem o telefone celular? Sem os satélites?

=== A Lei de Moore

Durante os anos de 1950 e 1965, a industrias do Vale do Silício 
disputavam pelo domínio do recém-surgido mercado da computação e 
eletrônica. Naquela época ainda não havia surgido o termo TIC 
(Tecnologia da Informação e Comunicação), mas ele seria mais 
apropriado para definir o nicho de clientes e serviços que eles 
disputavam. Eles dominavam a produção de circuitos eletrônicos 
digitais, dominados pela Intel e AMD, a produção de computadores e 
equipamentos de comunicação, como a Dell, Apple, IBM, HP e CISCO, 
além da indústria e software e serviços, como a Apple, Microsoft 
e, mais tarde, a Google. A disputa era grande e nem sempre leal.

[NOTE]
Assista ao filme ``Piratas do Vale do Silício'' (1999) e tenha uma 
ideia de como essa guerra estava longe de ser limpa. 

Entretanto, não se sabia naquela época onde essa disputa ia parar, 
nem quem seriam os vencedores, nem mesmo, se haveria sequer 
vencedores. Até um dos sócios e presidente da Intel, Gordon Moore,  
lançou um trabalho minucioso onde ele destacava a experiência que 
ele adquiriu ao longe de alguns anos trabalhando na indústria de 
fabricação de processadores e circuitos para computadores. Ele 
percebeu que, sempre a indústria avançava em sua tecnologia e 
conseguia reduzir o tamanho de cada transistor de um circuito 
integrado, os computadores tornavam-se também muito mais velozes do 
que antes. Porém, essa redução no tamanho dos transistores requer 
uma total atualização nos equipamentos da indústria, tornando os 
equipamentos anteriores obsoletos. Assim, só seria viável a 
evolução para transistores menores se o lucro da empresa fosse o 
suficiente para pagar todas essas despesas. Por outro lado, ele 
também percebeu que os computadores e equipamentos mais obsoletos 
ainda possuíam mercado aberto em países menos desenvolvidos 
economicamente. Ele concluiu então que a indústria seria sim capaz 
de continuar evoluindo na redução do tamanho dos transistores 
porque os novos computadores, sendo tornando mais velozes, seriam 
tão mais eficientes e atrativos, que todos os clientes, 
principalmente as empresas, fariam de tudo para trocar seus 
computadores antigos por novos, afim de se tornarem cada vez mais 
produtivos.

Além dessa análise de mercado, ele analisou como o processo 
industrial era concebido e como os novos computadores se 
beneficiariam da redução do tamanho dos transistores. Ao final, ele 
afirmou que “A cada ano a quantidade de transistores por chip irá 
dobrar de tamanho, sem alteração em seu preço”. Essa frase pode 
ser interpretada também pelas consequências da quantidade de 
transistores por chip. Ou seja, a cada ano, com o dobro dos 
transistores, os chips se tornarão duas vezes mais rápidos. Um 
exemplo mais comum de chip são os processadores dos computadores. 
Então, por consequência, os computadores irão dobrar sua 
velocidade de processamento a cada ano, e ainda vão permanecer com o 
mesmo preço. 

Naquela época essa era uma afirmação muito forte e ambiciosa. 
Muitos receberam esse estudo com cautela. Mas não demorou muito para 
todos perceberem que as previsões de Moore estavam se realizando. 
Foi tanto, e o trabalho dele foi depois chamado de “Lei de Moore” 
e ela ainda é válida até os dias de hoje. Na <<fig_moore>> a 
seguir é possível perceber como a quantidade de transistores por 
processadores cresceu dos anos 1970 até por volta de 2003 (linha 
contínua). É possível ver que ela não se afastou muito das 
previsões de Moore (linha tracejada).

[[fig_moore]]
.Evolução do número de transistores nos processadores em comparação com a Lei de Moore
image::images/fig_moore.jpg[scaledwidth=“30%"]

A Lei de Moore se tornou tão importante que ela não é usada apenas 
como uma meta a ser buscada e batida a cada ano, mas também como um 
meio para se verificar se a indústria está evoluindo na velocidade 
esperada. Apesar de Moore está muito correto em suas previsões, 
todos sabem, inclusive ele próprio, que esse crescimento não vai 
durar para sempre. Os transistores hoje estão na escala de 25 
nanometros. Essa é a mesma escala de alguns vírus e bactérias. 
Reduzir mais do que isso está se tornando cada vez mais difícil. 
Pesquisadores e cientistas buscam outras formas de fazer com que os 
computadores continuem evoluindo em sua velocidade e reduzindo seu 
tamanho. Alguns pensam na substituição de transistores de Silício 
por outros materiais, como Grafeno. Outros até são mais radicais e 
defendem que a forma de computação deve mudar, talvez através de 
Computadores Quânticos ou de Bio-Computadores.

Quanto menores forem os transistores, mais rapidamente eles podem ser 
carregados e descarregados. Isso possibilita que o sistema trabalhe 
cada vez mais veloz. Mas há ainda outra limitação para a redução 
do tamanho dos transistores é a dissipação de calor. Quanto 
menores os transistores, mais deles são adicionados num mesmo 
circuito. O funcionamento dos transistores, como dito anteriormente, 
é feito através da passagem de corrente elétrica (elétrons em 
movimento). Como toda máquina elétrica, nem toda corrente é 
aproveitada. Muito dela é desperdiçada através da dissipação de 
calor. Então, uma vez que há milhões desses transistores 
trabalhando juntos, a dissipação de calor é ainda maior.

É muito importante para toda a humanidade que os computadores 
continuem evoluindo. A redução do tamanho dos computadores, aliada 
ao aumento de desempenho e sem o crescimento dos preços, permitiu 
que todas as ciência evoluíssem ao mesmo tempo, com a mesma 
velocidade. A metereologia, a medicina, as engenharias e até as 
Ciências Humanas avançaram sempre em conjunto com o avanço da 
computação. Para se ter um exemplo, foi a evolução dos 
transistores que permitiu que computadores se comunicassem numa 
velocidade tão grande que permitiu a formação da rede mundial de 
computadores, a Internet. Qualquer pessoa hoje consegue em poucos 
milissegundos fazer uma pesquisa por informações que estão do 
outro lado do planeta. Algo que antes só era possível viajando até 
bibliotecas distantes e cheirando bastante mofo e poeira. Hoje, ter 
em casa bilhões de bytes (Giga bytes) armazenados num minúsculo 
cartão de memória, é algo corriqueiro. A informação está hoje 
disponível numa escala tão grande e numa velocidade tão intensa 
que parece que mais nada é impossível para a humanidade. Após a 
Revolução Industrial do século XVIII que substitui os 
trabalhadores braçais por máquinas, o século XX, puxado pela 
evolução dos transistores, passou pelo o que muitos consideram a 
Revolução da Informação e o século XXI, já é considerado a 
“Era do Conhecimento”.


=== A evolução dos computadores

==== O ENIAC

O primeiro computador criado foi o ENIAC (‘Electronic Numerical 
Integrator And Computer’), desenvolvido por Eckert e Mauchly na 
Universidade da Pennsylvania, Estados Unidos. O projeto iniciou em 
1943 financiado pelo governo americano. O período era da Segunda 
Guerra Mundial e o objetivo era poder calcular de forma mais ágil as 
melhores trajetórias para transporte de armas e mantimentos em meio 
aos exércitos inimigos. Esse é o tipo de cálculo que pequenos 
aparelhos celulares fazem hoje para encontrar rotas nas cidades 
através de GPS (‘Global Positioning System’) e análise de mapa. 
O projeto só foi concluído em 1946, tarde demais para ser utilizado 
para a Segunda Guerra, mas foi bastante utilizado até 1955.

O ENIAC ocupava uma área de 4500 metros quadrados, pesava 30 
toneladas e consumia cerca de 140KW. Ele era capaz calcular 5000 
somas por segundo. A programação era feita manualmente através da 
manipulação de chaves, ou seja, não havia linguagem de 
programação, nem compiladores ou interpretadores de comandos. O 
Sistema Operacional só surgiu bem depois e tomou o emprego de muitos 
funcionários chamados na época de operadores de computadores. 
Profissão hoje extinta! O ENIAC ainda não utilizada transistores, 
mas válvulas que, dependendo de seu nível de carga, representavam 
um número. Cada válvula precisava estar devidamente aquecida para 
funcionar corretamente, então o processo de ligar o ENIAC era 
trabalhoso e levava bastante tempo. Ele trabalhava com o sistema de 
numeração decimal, o que parecia óbvio naquela época, mas que 
depois dos transistores, se tornaram complexo demais e foi adotado o 
sistema binário.

Após a Segunda Guerra iniciou-se o período chamado de Guerra Fria, 
quando a espionagem, sabotagem e muito especulação reinava entre os 
países liderados pela União Sovitética e Estados Unidos. Prato 
cheio para os computadores. Possuir um computador que fosse capaz de 
decifrar mensagens codificadas dos inimigos era o sonho de consumo de 
todo general daquela época. 

==== A Arquitetura de von Neumann

Muitas empresas e governos corriam para construir seu próprio 
computador que fosse mais avançado do que os anteriores. Muitos 
projetos surgiram depois do ENIAC. Mas todos eles eram barrados por 
algumas dificuldades e limitações. Como por exemplo, o fato de não 
serem programados e trabalharem com números decimais. O problema de 
trabalhar com decimais é que cada algarismo armazenado possui 10 
estados possíveis, representando os números de 0 a 9. Dentro de um 
sistema eletrônico, isso é complicado por que a carga de cada 
dispositivo, seja transistor, seja válvula, deveria ser medida para 
verificar se que número ela estava representando. Os erros eram 
muito frequentes. Bastava que uma válvula estivesse fora da 
temperatura ideal para que os resultados das operações começassem 
a sair errado.  Von Neumann recomendou em sua arquitetura que os 
dados e instruções fossem agora armazenados em binário, 
facilitando a análise dos mesmos e reduzindo a quantidade de erros.

Em 1952, o professor John von Neumann, da Univeridade de Princeton, 
Estados Unidos, apresentou um projeto inusitado para a arquitetura de 
um computador. Ele sugeriu que o computador fosse organizado em 
componentes, cada um executando apenas uma única tarefa e de forma 
muito mais organizada. Ele propôs que o computador fosse composto 
por (ver <<fig_neumann>>):

* Memória Principal: responsável por armazenar os programas a serem 
executados, assim como os dados a serem processados
* Unidade Lógica e Aritmética (ULA): para realização das 
operações lógicas e aritméticas
* Unidade de Controle: que, baseado nas instruções lidas da 
memória, enviaria sinais de controle para a ULA para que a mesma 
executasse as operações devidas
* Unidade Central de Processamento (CPU): que agruparia a ULA e a 
Unidade de Controle
* Unidade de Entrada e Saída: responsável pela comunicação com os 
periféricos do computador (teclado, monitor, memória externa etc.)

[[fig_neumann]]
.Estrutura da Máquina de von Neumann
image::images/fig_neumann.jpg[scaledwidth=“30%"]

A Arquitetura de von Neumann deu tão certo que todos os fabricantes 
começaram a segui-la. Os computadores utilizados até hoje em dia 
seguem os preceitos básicos propostos por ele. Muitos avanços 
surgiram, melhorias foram feitas, mas até hoje os computadores são 
formados por Unidades de Controle, CPU, ULA, memória e Unidades de 
Entrada e Saída. John von Neumann deixou um legado para toda a 
humanidade.


==== A IBM
 
A International Business Machines, ou apenas IBM, foi fundada em 1911 
com o nome de Computing Tabulating Recording (CTR) e iniciou 
produzindo e comercializando calculadoras para empresas e 
empresários. Só em 1924 é que ela muda de nome para International 
Business Machines ou apenas IBM. Ela é uma das poucas empresas que 
sobreviveram a todos os avanços da computação e continua sendo uma 
potência mundial. Apenas em 1953 a IBM entra no mercado de 
computadores com o IBM 701, tendo sempre as grande organizações 
como alvos. Só muitos anos depois é que os computadores pessoais 
foram surgir. O IBM 701 trabalhava com cartões perfurados, ou seja, 
toda programação dele era feita através de uma perfuradora que 
marca buracos para representar o bit 1, e deixava ilesa uma área 
para representar o 0. O mesmo acontecia depois que os programas era 
lidos e processados. Uma folha de papel era perfurada pelo computador 
para representar o resultados das operações executadas. Não 
preciso nem dizer o que isso era trabalhoso!

Em 1955 a IBM lança o IBM 702 que agora não fazia apenas cálculos 
científicos, mas também aplicações comerciais, visando deixar de 
ser um equipamento apenas para cientistas, mas também para 
empresários. Depois desses vários outros computadores foram 
lançados nas séries 700. Essas máquinas ainda utilizavam válvulas 
para armazenar os dados. Só em 1957 é que surge a Segunda Geração 
de computadores, com a utilização de transistores. Isso tornou os 
computadores mais leves, baratos, velozes e mais energicamente 
eficientes. Os primeiros computadores dessa geração foram o IBM 
7000 e o PDP-1, da DEC, empresa que não existem mais.


A IBM lança em 1964 o IBM série 360, substituindo os antigos 
computadores da série 7000. O IBM 360 inicia a primeira família de 
planejada de computadores. Isso significava que todos computadores 
seguintes da série 360 seriam compatíveis com os anteriores. Todos 
programas desenvolvidos ou adquiridos pelas empresas poderiam ser 
usados mesmo que a empresa substituísse os computadores pela 
geração mais nova. Isso tornou a IBM uma das empresas mais 
poderosas do mundo na época, com filiais e representantes em todos o 
continentes do planeta.

==== As gerações dos computadores

As gerações de computadores surgiram com a miniaturização dos 
transistores e sua integração em chips em escalas cada vez maiores. 
Podemos então ver as gerações dos computadores como:

* 1946 a 1957: computadores baseados em tubos de vácuo
* 1958 a 1964: surgimento dos transistores
* 1965: indústrias atingiram a integração de até 100 transistores 
num único chip
* 1971: chamada de Integração em Média Escala, com até 3000 
transistores por chip
* 1971 a 1977: Integração em Larga Escala, com até 100.000 
transistores por chip
* 1978 a 1991: Integração em Escala Muito Grande (VLSI), com até 
100 milhões de transistores por chip
* 1991 até a atualidade: Integração Ultra-VLSI, com mais de 100 
milhões de transistores por chip

==== Memórias de semicondutores

Em 1970, uma empresa chamada Fairchild desenvolveu pela primeira vez 
uma memória utilizando a mesma tecnologia utilizada para fabricar os 
processadores, os transistores. Isso possibilitou que memórias muito 
menores, mais rápidas e mais baratas fossem desenvolvidas. E melhor, 
elas poderiam ser inseridas muito próximas, e até dentro dos 
processadores, acompanhando sua miniaturização. E foi o que 
aconteceu. A medida que a tecnologia foi avançando e produzindo 
transistores cada vez menores, as memórias também foram encolhendo.

Os processadores tornaram-se cada vez menores e mais velozes, mas 
infelizmente o avanço não ocorreu também com a velocidade das 
memórias, mas apenas com o seu tamanho. Isso até hoje é um 
problema. Armazenamentos rápidos são muito complexos de fabricar e, 
por consequência, caros. Isso vem limitando o avanço da velocidade 
dos computadores, mas sempre os cientistas vêm encontrando 
alternativas para manter Gordon Moore e todos nós muito orgulhosos.

==== A Intel

A Intel Corporation, ou simplesmente Intel, surgiu nos Estados Unidos 
em 1968, como uma empresa focada no projeto e fabricação de 
circuitos integrados. Ela foi fundada por Gordon Moore (o mesmo da 
Lei de Moore) e Robert Noyce. Ela era inicialmente uma concorrente da 
IBM, mas logo se tornaram parceiras. A Intel fabricava os 
processadores e memória, e a IBM fazia a composição deles com 
outros componentes para montar os computadores. 

Em 1971 a Intel lança seu primeiro processador, o 4004, que 
trabalhava com operações e dados de 4 bits. Foi uma revolução, 
pois todos componentes da CPU estavam num único chip. No ano 
seguinte eles lançam o 8008, já de 8 bits. Em 1974 é lançado o 
8080, primeiro processador de propósito geral. Ou seja, com ela 
tanto era possível executar aplicações científicas, financeiras, 
gráficas e jogos. O mesmo princípio dos processadores atuais. Ele 
foi substituído pelo 8086 de 16 bit. O próximo foi o 80286 que já 
era capaz de trabalhar com uma memória de 16MBytes. O 80386 
trabalhava com 32 bits e tinha suporte a multi-tarefas, ou seja, era 
finalmente possível executar mais de uma aplicação 
simultaneamente. Depois veio o 80486 com muito mais memória e bem 
mais rápido, além de um co-processador específico para 
aplicações matemáticas. A partir do 80286 as pessoas omitiam o 80 
ao falar do processador, chamando-o apenas de 286, 386 e 486.

Em seguida veio a geração Pentium, focando cada vez mais na 
execução de tarefas paralelas, adicionando várias unidades de 
processamento e armazenamento de dados dentro processador. Agora os 
processadores não teriam apenas uma ULA ou uma memória dentro do 
processador, mas várias delas. Hoje estamos na geração dos 
processadores multi-núcleos, ou multi-cores, que nada mais são do 
que vários processadores replicados dentro de um mesmo chip e 
coordenadores por uma unidade única.


==== A Apple e a Microsoft

Em 1974 Steve Jobs e Steve Wosniak trabalhavam noites a fio para 
tentar, pela primeira vez, criar um computador que fosse voltado não 
a empresas, mas a pessoas também. Seria a ideia de um computador 
pessoal. Eles compraram todos componentes necessários para montar um 
computador, fizeram várias improvisações e inovações, acoplaram 
uma TV e um teclado. Wosniak, um gênio da eletrônica e 
programação, desenvolveu o software para controlar o computador e 
ainda alguns aplicativos, como uma planilha de cálculos e alguns 
jogos. Assim que o protótipo ficou prontos, Steve Jobs, eximiu 
negociador e vendedor, colocou o computador na mala de seu carro e 
foi visitar várias empresas para conseguir algum apoio financeiro 
para poder fabricar o produto em escalas maiores. Foi até na IBM, 
mas ouviu deles que o mercado de computadores pessoais não era 
promissor e o negócio deles era a produção de grandes computadores 
para empresas.

Assim que conseguiram o primeiro cliente, em 1976, Jobs e Wosniak 
fundaram a Apple e lançaram o Apple I. Um produto mais maduro e 
melhor acabado. Jobs sempre gostava de produtos de design 
diferenciado, que fossem não apenas eficientes, mas bonitos e, 
principalmente, fáceis de usar. Suas apresentações anuais de 
lançamento de novos produtos eram sempre aguardados com grande 
expectativa e especulações.

A IBM inicialmente também desenvolvia o Sistema Operacional e os 
programas que iriam ser executados por suas máquinas. Logo ela 
percebeu que poderia fazer parcerias com outras empresas e agregar 
ainda mais valor aos seus produtos. Foi aí que surgiu a Microsoft, 
liderada pelo seu fundador, Bill Gates, com o seu sistema 
operacionais MS-DOS. Não demorou muito para que todos computadores 
lançados pela IBM trouxessem também o MS-DOS integrados e eles. 
Depois surgiram as evoluções do MS-DOS, o Windows e suas várias 
gerações. A Microsoft se beneficiou bastante dessa parceria, já 
que todos a grande maioria dos computadores do mundo executavam seu 
sistema, as pessoas teriam que aprender e se familiarizar com seu 
sistema operacional. As empresas de desenvolvimento de aplicativos e 
jogos tinham que fazê-los compatíveis com o MS-DOS e Windows e foi 
aí que a Microsoft se tornou uma das líderes do mercado e, por 
muitos anos, a mais rica empresa do mundo.

Steve Jobs sempre acusou o Bill Gates de ter copiado dele o código 
principal para o funcionamento do primeiro sistema operacional 
Windows. Gates nunca negou. Eles sempre trocavam acusações e isso 
gerou muito assunto para a impressa e fanáticos por tecnologia. A 
verdade é que a Microsoft cresceu bastante e a Apple passou por 
vários apertos. Só no ano 2000, quando Jobs retornou à Apple 
depois de ter sido expulso da própria empresa que ele fundou, foi 
que as coisas melhoraram para a Apple. Eles lançaram produtos em 
outras linhas que não fossem computadores pessoais, como o iPod para 
ouvir música e o telefone celular iPhone. A Apple passou então a 
dominar o mercado de música online com sua loja de músicas, iTunes 
e o iPhone é o Smartphone mais vendido do mundo.

Steve Jobs seguia a filosofia não de fazer clientes, mas de criar 
fãs. E deu certo. Hoje há vários ``Apple Maniamos” que compram 
seus produtos antes mesmo deles serem apresentados ao público. Nos 
dias em que esse livro está sendo escrito, a Apple ultrapassou a IBM 
e a Microsoft em valor, e é a empresa mais valiosa do mundo.


=== Recapitulando

Ao final desse capítulo vimos o que é a arquitetura de um 
computador e porque é tão importante estudá-la. Vimos que o 
transistor é o dispositivo básico para todo o funcionamento de um 
computador. Estudar seu funcionamento e sua evolução, é estudar a 
própria Ciência da Computação e a eletrônica digital. Depois de 
seu surgimento, os computadores foram possíveis e avançaram a 
medida que eles encolhiam de tamanho e aumentavam de velocidade, 
consumindo menos energia. Com a evolução dos computadores, cada vez 
mais rápidos, menores e mais baratos, toda a humanidade avançou na 
mesma velocidade. No próximo capítulo vamos estudar mais a fundo 
como os processadores funcionam. Como os programas são executados e 
o que é feito nos dias de hoje para que eles sejam cada vez mais 
eficientes.

=== Atividades

* Quais as quatro funções básicas que todos os computadores 
executam? Dê um exemplo de operação de cada uma delas.
* Quais os elementos básicos de um computador e quais as 
funcionalidades de cada um deles?	
* Quais as diferenças entre um sinal analógico e um digital? 
Apresente os pontos fortes e fracos de cada um deles. Na sua 
opinião, qual dos dois sinais apresentam maior qualidade?
* Caracterize o que é uma Máquina de von Neumann
* O que são transistores? Quais as vantagens na concepção de 
computadores com o surgimento dos transistores?
* Por que quantos menores os transistores, mais velozes os 
computadores? Há desvantagens nessa miniaturização das máquinas? 
Quais?
* O que diz a Lei de Moore? Em sua opinião, há um limite para esse 
crescimento? Onde vamos chegar?
* Que outras técnicas podem ser utilizadas para aumento do 
desempenho dos processadores que não pela redução do tamanho dos 
transistores? Explique cada uma delas.

// Sempre terminar o arquivo com uma nova linha.

