== Sistema de Memória

:cap: cap5
:online: {gitrepo}/blob/master/livro/capitulos/code/{cap}
:local: {code_dir}/{cap}
:img: {image_dir}/{cap}

.Objetivos do capítulo
____________________
Ao final deste capítulo você deverá ser capaz de:

* Conhecer o Sistema de Memória e seus componentes;
* Descrever as principais características das tecnologias utilizadas
  para memórias primárias e secundárias;
* Apresentar conceitos detalhados sobre as Memórias Cache e seu
  funcionamento; 
____________________

Sistema de Memória é uma das principais partes do computador,
juntamente com o processador.  Todos programas e seus dados são
mantidos no Sistema de Memória e ele é responsável por entregar o mais
rapidamente para o processador quando solicitado.  Não é uma tarefa
simples porque as memórias tendem a ser muito mais lentas do que o
processador e sua tecnologia não tem avançado tão rapidamente quanto a
dos processadores.  Neste capítulo vamos entender um pouco mais sobre
esse sistema e como ele apoia o trabalho dos processadores em busca de
sistemas cada vez mais eficientes.

=== Introdução

Em todo sistema computacional, a memória é componente essencial e de
extrema relevância para o bom funcionamento do computador.  Com o
passar dos anos, as memórias evoluíram bastante e são formadas por
vários componentes numa chamada Hierarquia de Memória.  Na
<<fig_hierarquia>> é apresentada como o ela é organizada.  As memórias
de mais velozes possuem custo por bit maior, devido às suas
tecnologias, mais avançadas e mais complexas para fabricação

[[fig_hierarquia]]
.Hierarquia de Memória
image::images/cap5/fig_hierarquia.jpg[scaledwidth=“100%”]


As tecnologias mais avançadas até o momento são as chamadas SRAM
(Static Random-Access Memory).  Elas são mais utilizadas em
registradores e memórias Cache.  Por serem mais caras, elas estão
presentes nos computadores em quantidades menores, para não encarecer
demais o projeto.  Já a memória principal é fabricada utilizando
tecnologia DRAM (Static Random-Access Memory).  Por serem de
tecnologia menos sofisticada, são mais lentas, mas mais baratas do que
as SRAM.  Por isso elas são montadas em maior quantidade do que as
memórias Cache e os registradores.


Já as Memórias Secundárias são formadas por tecnologias de memórias
Magnéticas e Ópticas.  Suas principais características são o baixo
preço por bit, baixo preço e, por consequência, alta capacidade.  Além
disso, as Memórias Secundárias são memórias não voláteis, ou seja,
seus conteúdos são preservados mesmo com a interrupção da fonte de
energia.  Devido ao avanço da complexidade das memórias dos
computadores, elas são organizadas formando o chamado Sistema de
Memória.

=== Princípio da Localidade

Muitos dizem que o Sistema de Memória se inspirou no sistema de
memória do corpo humano, onde lembranças mais recentes são armazenadas
em memórias menores de curta duração e lembranças mais antigas e
pertinentes são armazenadas em memórias de longa duração e maior
capacidade.  No sistema computacional o Sistema de Memória se baseia
no Princípio da Localidade, que se divide em Temporal e Espacial.

O Princípio da Localidade Temporal diz que um dado acessado
recentemente tem mais chances de ser usado novamente, do que um dado
usado há mais tempo.  Isso é verdade porque as variáveis de um
programa tendem a ser acessadas várias vezes durante a execução de um
programa, e as instruções usam bastante comandos de repetição e
sub-programas, o que faz instruções serem acessadas repetidamente.
Sendo assim, o Sistema de Memória tende a manter os dados e instruções
recentemente acessados no topo da Hierarquia de Memória.

Já o Princípio da Localidade Espacial diz que há uma probabilidade de
acesso maior para dados e instruções em endereços próximos àqueles
acessados recentemente.  Isso também é verdade porque os programas são
sequenciais e usam de repetições.  Sendo assim, quando uma instrução é
acessada, a instrução com maior probabilidade de ser executada em
seguida, é a instrução logo a seguir dela.  Para as variáveis o
princípio é semelhante.  Variáveis de um mesmo programa são
armazenadas próximas uma às outras, e vetores e matrizes são
armazenados em sequência de acordo com seus índices.  Baseado neste
princípio, o Sistema de Memória tende a manter dados e instruções
próximos aos que estão sendo executados no topo da Hierarquia de
Memória.


=== Funcionamento do Sistema de Memória

O ponto inicial da memória é a Memória Principal (por isso ela recebe
esse nome).  Todo programa para ser executado deve ser armazenado
nesta memória, com todo seus dados e instruções.

[NOTE]  
Mais a frente veremos que nem sempre é possível manter todos programas em execução na Memória Principal.

Devido ao Princípio da Localidade, sempre que o processador solicita
um dado/instrução da memória, ele e seus vizinhos (Localidade
Espacial) são copiados para a Memória Cache no nível superior a seguir
da hierarquia, Cache L2, por exemplo.  Uma parte menor deste bloco é
transferido para o nível seguinte (Cache L1, por exemplo), e uma parte
ainda menor (porções individuais) é transferida para registradores.

Quando o processador vai acessar um endereço de memória, ele faz uma
consulta no sentido inverso, do topo da hierarquia até a base.
Primeiro ele busca o conteúdo nos registradores.  Se não encontrar,
ele vai buscar no primeiro nível de Cache.  Se não encontrar, ele
busca no próximo nível de Cache e, por fim, na Memória Principal.

O grande problema é que os níveis superiores da Hierarquia de Memória
possuem capacidade cada vez menores a medida que se aproximam do topo.
Isso implica na falta de capacidade de armazenar todos dados e
instruções que estão sendo executadas pelo processador.  Por isso, o
sistema deve decidir o que é mais relevante e fica nos níveis mais
altos, e o que é menos relevante e deve ficar nos níveis inferiores da
hierarquia.

Perceba que tudo é uma questão de aposta.  Tudo o que o processador
possui a seu favor é o Princípio da Localidade, mas que se baseia em
probabilidade.  Há uma probabilidade de um endereço próximo (temporal
e espacialmente) a um que foi acessado, ser acessado também, mas não
há garantias.  Muitas vezes ele acerta, mas muitas outras ele erra, e
quem perde é o desempenho geral do sistema.

Como fazer então para aumentar a probabilidade de um endereço ser encontrado no topo da Hierarquia de Memória?
A resposta é simples, mas não barata!
Deve-se investir em registradores e memórias Cache maiores.

Quando as memórias estão cheias, o Sistema de Memória possui uma
tarefa difícil, que é remover um conteúdo considerado menos relevante
no momento, e substituir por um outro mais relevante naquele momento.
A única memória que continua com uma cópia de todos os conteúdos é a
Memória Principal.  A escolha de qual conteúdo deve ser removido se
baseia também no Princípio da Localidade, mas há diversas formas de
implementar o algoritmo de substituição de conteúdos, que também podem
influenciar no desempenho do sistema.

[NOTE] 
Veremos que a Memória Virtual quebra essa ideia de que a Memória
Principal sempre mantém cópia de todos programas.

=== Memórias de Semicondutores

As memórias de semicondutores são consideradas aquelas que utilizam
composição de transistores como forma principal de armazenamento.
Dentro deste grupo estão os registradores, memórias cache, memórias
principais e, mais recentemente, cartões de memória, pen-drives e os
chamados Discos de Estado Sólido (SSD), que não possuem formatos de
disco, mas receberam esse nome por serem os candidatos mais cotados a
substituírem os Discos Rígidos (HD).

Dentro das memórias de semicondutores vamos apresentar: 

* Random-Access Memory (RAM)
* Dynamic RAM (DRAM)
* Static RAM (SRAM)
* Synchronous Dynamic RAM (SDRAM)
* Double-Data Rate SDRAM (DDR-DRAM)
* Read-Only Memory (ROM)

==== Random-Access Memory (RAM)

O termo Random-Access Memory, ou RAM, ou Memória de Acesso Aleatório
em português, veio porque essa tecnologia substituiu as anteriores
memórias de Acesso Sequencial.  No Acesso Sequencial, os endereços são
acessados obrigatoriamente de forma sequencial, 0, 1, 2, 3,...  Essa é
a forma de acesso de memórias magnéticas, como fitas cassete e VHS, e
os discos rígidos (com alguma melhoria).

Já as memórias de acesso aleatório podem acessar qualquer endereço
aleatoriamente, independente de sua posição.  Hoje, o termo Memória
RAM é utilizado de forma errada para representar a Memória Principal,
mas na verdade, tanto registradores, quanto memória Cache e Memória
Principal são feitos utilizando tecnologia RAM.  Assim, RAM é uma
tecnologia e não uma memória.  A partir de hoje então, não utilize
mais memória RAM, mas Memória Principal quando se referir à principal
memória dos computadores.

==== Dynamic RAM (DRAM)

As memórias Dynamic RAM são as mais simples de serem fabricadas.  Como
mostrado na <<fig_dram>>, é formada simplesmente por um único
transistor e um capacitor.

[[fig_dram]]
.Estrutura de uma DRAM
image::images/cap5/fig_dram.png[scaledwidth=“80%”]

A figura apresenta uma memória de um único bit.  O transistor cuida de
abrir ou fechar a passagem de corrente para linha B.  Já a linha de
endereço é utilizada para fechar a porta do transistor e carregar o
capacitor.  Se o capacitor estiver carregado, é considerado que a
memória contém o bit 1.  Caso contrário, a memória contém o bit 0.

A simplicidade desta implementação traz resultado no seu principal
ponto negativo.  Assim como todo capacitor, o capacitor responsável
por manter a carga da memória só é capaz de manter a carga por um
curto tempo.  Aos poucos, a carga vai sendo dissipada, até o momento
em que era o bit 1, se torna 0, gerando um erro.  Para evitar isso, é
adicionado um circuito a parte que de lê o conteúdo da memória
periodicamente e recarrega todos capacitores que estão com bit 1.

Vamos lembrar que as memórias hoje estão na casa de Giga Bytes.  Ou
seja, bilhões de bytes.  Então, bilhões de capacitores devem ser lidos
e recarregados periodicamente para que os conteúdos não sejam
perdidos.  Esta técnica é chamada de *Refrescagem*.  Ela resolve o
problema dos dados perdidos, mas atrapalha bastante o desempenho da
memória.  Sempre que a Refrescagem precisa ser realizada, todo acesso
é bloqueado.  Nada pode ser lido ou escrito enquanto isso.  Assim, o
processador precisa esperar que o processo de refrescarem termine para
poder acessar novamente a memória.

Devido à sua simplicidade de fabricação, as memórias DRAM são mais
utilizadas para compor a Memória Principal, devido ao preço mais
acessível do que o das mais modernas SRAM.


==== Static RAM (SRAM)

As memórias RAM Estáticas (Static RAM ou SRAM) se baseiam na
composição de transistores	para que a carga para manter o bit 1 e
compartilhada entre outros transistores.  A <<fig_sram>> apresenta
essa composição de transistores.

[[fig_sram]]
.Estrutura de uma SRAM
image::images/cap5/fig_sram.png[scaledwidth=“80%”]

Nesta ilustração, o transistor T5 é que determina se o bit é 0 ou 1, e
os transistores, T1 e T3 são utilizados para recarregá-lo, caso sua
carga reduza.  Já os transistores T2, T4 e T6 são o complemento deles
de forma inversa, adicionando um nível a mais de segurança.  Essa
técnica é chamada Complementary MOS (CMOS).

As memórias SRAM não precisam de circuito de refrescarem, por isso,
não precisam parar e tornam-se muito mais rápidas do que as DRAM.  O
problema é que elas precisam de muito mais transistores por bit, o que
torna o projeto maior e, por consequência, mais caro.

Devido ao seu preço, elas são mais utilizadas em memórias Cache, mas
em menor quantidade do que as memórias principais.

==== Synchronous Dynamic RAM (SDRAM)

Já a Synchronous Dynamic RAM (SDRAM) é uma DRAM com um simples avanço.
O relógio que determina o tempo das SDRAM vem diretamente do
processador, e não de um relógio próprio, como nas DRAM convencionais.
Isso faz com que o momento exato da Refrescagem seja determinado pelo
processador.  Dessa forma, o processador sabe exatamente quando ele
não pode acessar a memória, e dedica seu tempo às outras tarefas, ou
seja, o processador não perde mais tanto tempo esperando a memória.

==== Double-Data Rate SDRAM (DDR-DRAM)

Após as SDRAM surgiram as DDR-SDRAM.  As memórias DDR são síncronas
como as SDRAM, mas elas possuem um barramento extra que faz com que, a
cada ciclo de clock da memória, o dobro de dados são transferidos.  As
memórias DDR e suas sucessoras são mais utilizadas para utilização
como memória principal.

==== Read-Only Memory (ROM)

As memórias ROM também possuem um nome criado há muitos anos e hoje é
um termo que não faz tanto sentido.  Em português significam Memória
Apenas de Leitura.  Isso porque as primeiras ROM eram escritas durante
a fabricação e não podiam mais ser modificadas.  Mas outras gerações
foram desenvolvidas que permitiram a escrita e tornou o termo ROM
antiquado.  Todas memórias ROM são não voláteis, ou seja, mantêm seu
conteúdo mesmo com a falta do fornecimento de energia elétrica.  São
tipos de memória ROM:

As memórias PROM (Programmable ROM):: são memórias que vem com a
conexões abertas de fábrica e precisam de uma máquina para que os
dados sejam escritos nelas.  Uma vez escritos, eles não podem mais ser
modificados.

Já as memórias EPROM (Erasable PROM):: se baseiam no mesmo princípio
das PROM, mas uma máquina especial que utiliza raios UV pode ser
utilizada para apagar todo seu conteúdo e escrever novamente. 

As memórias EEPROM (Electronically Erasable PROM)::  possuem o mesmo
princípio das PROM, mas a máquina utilizada para escrita e apagar é
eletrônica.  Isso permite que um computador, ou um máquina especial
seja utilizada para escrever nas memórias, as tornando muito mais
utilizadas.

Já as memórias Flash:: se baseiam no princípio das memórias EEPROM,
mas o processo de apagar é feito em blocos grandes, o que acelera
bastante o processo.

As memórias ROM são muito utilizadas na formação da BIOS dos
computadores e as memórias Flash são o princípio básico de cartões de
memória, pen-drives e memórias de estado sólido.

=== Memórias Secundárias

As memórias que vimos até o momento são chamadas de Memórias
Primárias, porque são usadas para o funcionamento básico e primário da
CPU.  Já as memórias secundárias são utilizadas para dar um suporte a
mais ao sistema, ampliando sua capacidade de armazenamento.  O
objetivo destas memórias é o de trazer mais capacidade, sem o intuito
de realizar operações muito velozes.  Se bem que as memórias virtuais
que veremos na próxima seção fez com que a demanda por memórias
secundárias mais velozes crescesse.  São as principais tecnologias
utilizadas como memórias secundárias:

* Memórias magnéticas
* Memórias ópticas
* Memórias de estado sólido

Memórias magnéticas:: Utilizam o princípio de polarização para
identificar dados numa superfície magnetizável.  Assim como num imã,
cada minúscula área da memória é magnetizada como sendo pólo positivo
ou  negativo (ou Norte e Sul).  Quando a região é polarizada com pólo
positivo, dizemos que ela armazena o bit 1, e armazena o bit 0, quando
a polarização for negativa.  O maior exemplo de memória magnética
utilizado hoje são os Discos Rígidos, ou do inglês Hard Disk (ou HD).

Memórias ópticas:: Armazenam seus dados numa superfície reflexiva.
Para leitura, um feixe de luz (LASER) é disparado contra um também
minúsculo ponto.  O feixe bate na superfície volta para um sensor.
Isso indicará que naquele ponto há o bit 0.  Para armazenar o bit 1,
um outro LASER entra em ação provocando um pequena baixa na região.
Com isso, ao fazer uma leitura no mesmo ponto, o feixe de luz ao bater
na superfície com a baixa será refletido mas tomará trajetória
diferente, atingindo um outro sensor diferente daquele que indicou o
bit 0.  Quando este segundo sensor detecta o feixe de luz, é dito que
o bit lido foi o 1.  O maior representante das memórias ópticas são os
CDs, DVDs e, mais recentemente os Blu-Ray.

Memórias de estado sólido (ou em inglês Solid State Disk - SSD):: São
memórias feitas com tecnologia Flash mas para ser usadas em
substituição ao Disco Rígido.  Em comparação com ele, a memória de
estado sólido é muito mais rápida, mais residente a choques e consome
menos energia.  Em contrapartida, as memórias de estado sólido são bem
mais caras.


=== Memória Virtual

Com o crescente aumento da quantidade e tamanho dos programas sendo
executados pelos processadores, surgiu a necessidade de cada vez mais
memória principal.  O problema, com já foi dito, é que as memórias
principais (basicamente DRAM) são caras.  Ao mesmo tempo, quando há
muito programas sendo executados ao mesmo tempo, há uma grande
tendência de haver muitos deles esquecidos, sem serem acessados.
Esses programas ocupam espaço da memória principal de forma
desnecessária.

Pensando nisso, foi criado o conceito de Memória Virtual, que nada
mais é do que a técnica de utilizar a Memória Secundária, geralmente
HD ou SSD, como uma extensão da Memória Principal.  Desta forma,
quando a memória principal está cheia e não há mais espaço para novos
programas ou dados, o sistema utiliza a memória secundária.  Tudo é
feito de forma automática pela Unidade de Gerência de Memória (ou
Memory Management Unit - MMU) presente nos processadores.  Assim, todo
dado que é acessado é antes buscado pela MMU na memória principal.  Se
ele não estiver lá, ela vai buscar na memória secundária, faz uma
cópia na memória principal e libera o acesso ao dado.

A principal técnica de Memória Virtual é a Paginação.  Na Paginação,
todos os dados são acessados através de páginas.  Isso facilita o
processo de organização e localização dos dados que estão na memória
principal e secundária.  Agora, ao invés de gerenciar palavra por
palavra, o sistema gerencia grandes blocos (geralmente de 64KB)
chamados de páginas.

=== Memória Cache

Como foi dito anteriormente, as memórias Cache vem tendo um papel cada
vez mais importante nos sistemas computacionais.  Inicialmente elas
nem existiam nos computadores.  Depois foram adicionadas fora do
processador e em pequena quantidade.  Em seguida elas foram levadas
para dentro do processador e hoje em dia ocupam entre 60% e 80% da
área do chip do processador.

O princípio básico das memória Cache é o de manter uma cópia dos dados
e instruções mais utilizados recentemente (Princípio da Localidade)
para que os mesmos não precisem ser buscados na memória principal.
Como elas são muito mais rápidas do que a memória principal, isso traz
um alto ganho de desempenho.

A <<fig_cache>> apresenta este princípio.  Todo dado a ser lido ou
escrito em memória pelo processador antes passa para a Cache.  Se o
dado estiver na Cache, a operação é feita nela e não se precisa ir até
a Memória Principal.  Caso contrário, um bloco inteiro de dados
(geralmente com 4 palavras de memória) é trazido da Memória Principal
e salvo na Cache.  Só então a CPU realiza a tarefa com o dado.

[[fig_cache]]
.Estrutura de uma SRAM
image::images/cap5/fig_cache.jpg[scaledwidth=“80%”]

Sendo assim, o desempenho do computador ao acessar um dado de memória
é probabilístico.  Para cada dado a ser acessado há uma probabilidade
dele estar na memória Cache.  Se isso ocorrer dizemos que houve um
*Cache Hit* e o sistema ganha muito tempo com isso.  Caso contrário,
ocorre uma *Cache Miss* e o desempenho é bastante prejudicado.  A
grande questão é, como fazemos para aumentar a probabilidade de um
determinado dado estar na memória Cache ao invés da Memória Principal?
Podemos também refazer esta pergunta de uma forma mais geral.  Como
aumentar a taxa de Cache Hit (ou diminuir a taxa de Cache Miss)?

Há três principais estratégias para isso.  São elas:

* Aumentar o tamanho da Memória Cache
* Mudar a função de mapeamento
* Mudar a política de substituição

Vamos estudar como cada uma delas funciona.

==== Tamanho

A grande dificuldade das memórias Cache é que elas sempre estão
presentes em menor quantidade do que a Memória Pricipal.  Geralmente
há a Memória Cache de um computador é 1.000 vezes menor do que a
Memória Principal.  Se você tem um computador com 4GB de Memória
Principal (não usa mais RAM para indicar este tipo de memória!), você
terá muita sorte se seu processador tiver 4MB de Memória Cache.

Como a Memória Cache trabalha armazenando cópias de dados da Memória
Principal, quanto maior for a Memória Cache, mais dados ela é capaz de
armazenar, sendo assim, maior a probabilidade do processador buscar
por um dado e ele estar na Cache.  Entretanto, é importante observar
que esse crescimento não é constante, muito menos infinito.  Veremos a
seguir que o ganho de desempenho com o aumento do tamanho da Cache
possui um limite.

==== Função de mapeamento

A função de mapeamento diz respeito a estratégia utilizada para
determinar onde cada dado da memória principal estará na Cache.  Ela
determina onde cada dado da Memória Principal será copiado na Cache
caso ele seja acessado.  Isso é muito importante porque o processador
vai seguir essa mesma estratégia para conseguir localizar se o dado
está, ou não na Cache.  Há três tipos de mapeamento:

* Mapeamento direto
* Mapeamento associativo
* Mapeamento associativo por conjunto

===== Mapeamento direto

Para entendermos a diferença entre os tipos de mapeamento, vamos fazer
uma analogia com uma sala de cinema.  Imagine que o cinema é a Memória
Cache e cada pessoa é um dado a ser armazenado na memória.  No
mapeamento direto cada pessoa (sócia daquele cinema) receberá uma
cadeira dedicada a ele.  Sempre que ele for ao cinema, deverá sentar
no mesmo lugar.  O problema é que a Memória Principal é muito maior do
que a Memória Cache, então não há cadeira para todos.  Para resolver,
cada cadeira é distribuída por várias pessoas, apostando que nem
sempre as pessoas que compartilham o mesmo número de cadeira irão
assistir ao mesmo filme no mesmo horário.  Mas quando isso acontece, a
pessoa que chegou por último não pode sentar em outra cadeira mesmo
estando livre.  A pessoa que chega depois toma o lugar da pessoa que
está sentada, porque no caso da memória Cache, o último sempre tem
preferência.  Imagine quanta confusão isso geraria nesse cinema!

O bom do mapeamento direto é porque ele é muito fácil de organizar e a
CPU encontra sempre seu dado muito facilmente.  No exemplo do cinema,
se alguém estiver querendo saber se uma pessoa está no cinema (na
Cache) ou não (na Memória Principal) basta saber o número da cadeira
dele e ir lá verificar se é ele quem está sentado. Isso acelera
bastante o trabalho de busca da CPU.  Mas se a memória Cache for muito
menor que a Memória Principal, haverá muitos blocos com mesmo código e
pode haver muito conflito de posição, reduzindo o desempenho.

Por exemplo, imagine uma Cache que armazena apenas 5 linhas (é o termo
utilizado o local onde um bloco da Memória Principal é salvo na
Cache), com numeração de 1 a 5.  A Memória Principal será mapeada da
seguinte forma, o bloco 1 será salvo na linha 1 da Cache, o bloco 2 na
linha 2 etc. até o bloco 5 que será salvo na linha 5.  Já o bloco 6 da
memória será salvo novamente na linha 1 da Cache, o bloco 7 na linha
2, bloco 8 na linha 3 etc.  Isso será feito até o que todos blocos da
Memória Principal tenham uma linha a ser armazenada.

Agora suponha que os seguintes blocos da Memória Principal sejam
acessados em sequência: 1, 5, 1, 10, 11, 5.  Como será o mapeamento e
quando ocorrerá Cache Hit e Cache Miss?

No início o bloco 1 é acessado mas ele não está na Cache, ocorre um
Cache Miss e a cópia é salva.  Então temos:

----

Cache hit: 0
Cache miss: 1
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  -   -   -   -

----

Em seguida o bloco 5 é acessado.  Ele não está na Cache, ocorre um
Cache miss e uma cópia é salva na posição 5.  Temos então:

----

Cache hit: 0
Cache miss: 2
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  -   -   -   5

----

No terceiro acesso, o bloco 1 é buscado.  Ele já consta na Cache.
Enão ocorre um cache hit e a cache não precisa ser alterada.  Ficando
assim:

----

Cache hit: 1
Cache miss: 2
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  -   -   -   5

----

Ao acessar em seguida o bloco 10 é acessado, como ele deve ocupar
mesma posição do bloco 5 (isso porque 10 - 5 = 10), há um cache miss,
o 5 é removido e substituído pelo 10.

----

Cache hit: 1
Cache miss: 3
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  -   -   -   10

----

No próximo acesso o bloco 11 é buscado na posição 1 porque 11 - 5 = 6
e 6 - 5 = 1.  Ele não é encontrado, mas sim o bloco 1.  Há um cache
miss e o bloco 1 é substituído pelo 11, resultado no seguinte:

----

Cache hit: 1
Cache miss: 4
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache: 11  -   -   -   10

----

Por último, o bloco 5 é buscado novamente, mas o bloco 10 é quem ocupa
esta posição.  Há um cache miss e o bloco 10 é substituído pelo 5.
Como resultado final, temos:

----

Cache hit: 1
Cache miss: 5
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache: 11  -   -   -   5

----

===== Mapeamento associativo

No mapeamento associativo, o mecanismo de alocação de blocos da
Memória Principal na Cache não segue posição fixa.  Cada bloco vai
ocupar a primeira posição vazia encontrada.  Voltando ao exemplo do
cinema, seria uma sala sem cadeira reservada, mas com um porém.  Se
uma pessoa chegar e o cinema estiver cheio, a direção do cinema (no
computador é o Sistema de Memória) vai escolher uma pessoa a ser
removida para dar lugar a nova pessoa que chegou (talvez alguém que
estiver dormindo ou conversando durante o filme).

O mapeamento associativo termina sendo mais eficiente do que o
mapeamento direto no momento de alocar blocos da memória na Cache.  Só
haverá espaço inutilizado se não houver acesso suficiente à Memória
Principal.  A desvantagem deste tipo de mapeamento está no momento de
buscar um bloco na Cache.  Imagine agora que alguém chegue no cinema
cheio a procura de uma pessoa.  Como encontrá-la?  Será necessário
percorrer todas cadeiras para verificar se a pessoa se encontra em
alguma delas.  Para o sistema computacional, essa busca é custosa o
que resulta na utilização deste mapeamento apenas se a Cache não for
grande demais.

Agora vamos voltar ao mesmo exemplo de acesso à uma memória Cache de 5
linhas para a mesma sequência de acesso: 1, 5, 1, 10, 11, 5.  Como
será o mapeamento associativo e quando ocorrerá Cache Hit e Cache
Miss?

No início o bloco 1 é acessado mas ele não está na Cache, ocorre um
Cache Miss e a cópia é salva.  Sempre há cache miss nos primeiros
acessos de um programa e eles são impossíveis de serem evitados.
Então temos:

----

Cache hit: 0
Cache miss: 1
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  -   -   -   -

----

Em seguida o bloco 5 é acessado e há novamente um cache miss, mas
dessa vez vamos adicioná-lo na primeira posição livre que
encontrarmos.  Neste caso, na posição 2.  Temos então:

----

Cache hit: 0
Cache miss: 2
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  5   -   -   -

----

No próximo acesso ao bloco 1 há um cache hit porque o bloco 1 é
acessado e ele já está presente na Cache:

----

Cache hit: 1
Cache miss: 2
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  5   -   -   -

----

Em seguida o bloco 10 é acessado.  Ele não está na Cache e ocorre um
Cache Miss e ele é salvo na posição 3.

----

Cache hit: 1
Cache miss: 3
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  5   10   -   -

----

No próximo passo o bloco 11 é acessado.  Ele também não está na Cache
e é salvo na posição 4.

----

Cache hit: 1
Cache miss: 4
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  5   10  11   -

----

No último acesso o bloco 5 é acusado novamente.  Como ele está na
Cache, há um cache hit e a cache não é modificada.

----

Cache hit: 2
Cache miss: 4
                
Posição da Cache: 1  2   3   4   5
Linhas na Cache:  1  5   10  11   -

----

Perceba que ao final, o mesmo exemplo com Mapeamento Associativo teve 1
cache miss a menos do que o Mapeamento Direto, ou seja, ele foi mais
eficiente para esse exemplo, já que precisou ir menos à Memória
Principal mais lenta para trazer os blocos.  Note também que a memória
Cache permanece mais utilizada quando o mapeamento associativo é
aplicado.  Isso aumenta bastante a probabilidade novos cache hit.

===== Mapeamento associativo por conjunto

O problema do Mapeamento Associativo é encontrar blocos em memórias
Cache grandes.  A solução para isso é utilizar uma abordagem mista,
que utiliza os princípios dos mapeamentos direto e associativo.  Ela
divide a memória em conjuntos.  Cada bloco então é mapeado para um
conjunto (semelhante ao que é feito para o Mapeamento Direto, mas para
o nível de conjunto).  Sempre que um bloco for ser buscado ou salvo,
ele será feito no conjunto fixo dele, mas dentro do conjunto ele pode
ser armazenado em qualquer posição livre.

Voltando ao cinema, é como se uma grande sala fosse dividida em salas
menores.  Cada pessoa teria no seu ingresso o número da sala, mas a
poltrona seria escolhida livremente.  Escolhendo a quantidade certa e
o tamanho das salas, é possível utilizar bem os espaços e facilitar o
processo de busca por uma pessoa.

==== Política de substituição

Nos mapeamentos associativo e associativo por conjunto uma outra
política deve ser adotada.  Quando a memória cache enche e um novo
bloco precisa ser armazenado, o Sistema de Memória deve escolher que
bloco deve ser removido para dar espaço ao novo bloco.  No mapeamento
direto isso não existe porque cada bloco sempre fica na mesma posição.

Sendo assim, há 3 principais políticas de substituição de linhas de
Cache.  São elas:

* Randômica
* FIFO
* LRU

Na substituição randômica o sistema simplesmente escolhe
aleatoriamente o bloco que deve ser removido.  Ele sai da Cache dando
lugar ao novo bloco que foi acessado.  Este método tem a vantagem de
ser muito fácil de implementar e, por consequência, rápido de
executar.  Porém ele pode não ser muito eficiente.

Já no FIFO (First-In First-Out) adota o princípio de fila.  Aquele
bloco que chegou primeiro, está há mais tempo na Cache.  Já se
beneficiou bastante e deve então dar lugar ao novo bloco.

No LRU (Least-Recently Used), ou “Menos Usado Recentemente” aplica o
Princípio da Localidade Temporal e torna-se por isso mais eficiente na
maioria dos casos.  Nesta política o sistema escolhe o bloco que menos
foi utilizado recentemente e o remove.  Isso faz com que fiquem na
Cache aqueles blocos que são acessados mais vezes nos últimos
instantes.


=== Recapitulando

Neste capítulo foi apresentado os principais aspectos do principal
componente do computador depois do processador, o Sistema de Memória.
Vimos que a memória é tão complexa e com tantos elementos que ela é
organizada e considerada como um sistema por si só.  Foram
apresentadas as memórias primárias e suas características, as memórias
secundárias e, por fim, foi melhor detalhada a memória Cache, tão
importante para os sistemas computacionais modernos.

Com o entendimento dos conteúdos visto até o momento, mais o do
Sistema de Memória podemos dizer que o conhecimento introdutório da
Arquitetura de Computadores foi atingido.  Cabe a você agora explorar
novos caminhos.  Boa sorte!

// Sempre terminar o arquivo com uma nova linha.

